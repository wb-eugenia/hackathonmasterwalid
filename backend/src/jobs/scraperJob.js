/**
 * Job de scraping automatique des avis
 * S'exÃ©cute via un scheduler (cron) pour scraper rÃ©guliÃ¨rement les avis
 */

import cron from 'node-cron';
import { scrapeAllReviews } from '../services/scraperService.js';
import { Review } from '../models/Review.js';
import { Establishment } from '../models/Establishment.js';
import { analyzeSentiment, categorizeReview } from '../services/aiService.js';

/**
 * Scrape les avis d'un Ã©tablissement et les stocke en base
 */
export async function scrapeEstablishmentReviews(db, establishmentId) {
  try {
    const establishmentModel = new Establishment(db);
    const establishment = await establishmentModel.findById(establishmentId);

    if (!establishment || !establishment.google_place_id) {
      console.warn(`âš ï¸ Ã‰tablissement ${establishmentId} sans Google Place ID`);
      return;
    }

    console.log(`ğŸ” Scraping des avis pour ${establishment.name}...`);

    // Scrape les avis
    const scrapedReviews = await scrapeAllReviews(establishment.google_place_id);

    if (scrapedReviews.length === 0) {
      console.log(`â„¹ï¸ Aucun avis trouvÃ© pour ${establishment.name}`);
      return;
    }

    // Analyse chaque avis avec IA
    const reviewsWithAnalysis = await Promise.all(
      scrapedReviews.map(async (review) => {
        const [sentiment, category] = await Promise.all([
          analyzeSentiment(review.text),
          categorizeReview(review.text),
        ]);

        return {
          ...review,
          sentiment,
          category,
          establishmentId,
        };
      })
    );

    // VÃ©rifie les avis existants pour Ã©viter les doublons
    const reviewModel = new Review(db);
    const reviewsToInsert = [];

    for (const review of reviewsWithAnalysis) {
      if (review.googleReviewId) {
        const existing = await reviewModel.findByGoogleReviewId(review.googleReviewId);
        if (!existing) {
          reviewsToInsert.push(review);
        }
      } else {
        reviewsToInsert.push(review);
      }
    }

    // InsÃ¨re les nouveaux avis
    if (reviewsToInsert.length > 0) {
      await reviewModel.createBatch(reviewsToInsert);
      console.log(`âœ… ${reviewsToInsert.length} nouveaux avis ajoutÃ©s pour ${establishment.name}`);
    } else {
      console.log(`â„¹ï¸ Aucun nouvel avis pour ${establishment.name}`);
    }
  } catch (error) {
    console.error(`âŒ Erreur lors du scraping pour l'Ã©tablissement ${establishmentId}:`, error);
  }
}

/**
 * Scrape tous les Ã©tablissements actifs
 */
export async function scrapeAllEstablishments(db) {
  try {
    const establishmentModel = new Establishment(db);
    // RÃ©cupÃ¨re tous les Ã©tablissements avec un Google Place ID
    const establishments = await db.prepare(
      'SELECT id FROM establishments WHERE google_place_id IS NOT NULL'
    ).all();

    console.log(`ğŸ” DÃ©but du scraping pour ${establishments.results.length} Ã©tablissements...`);

    for (const establishment of establishments.results) {
      await scrapeEstablishmentReviews(db, establishment.id);
      // Pause entre chaque Ã©tablissement pour Ã©viter le rate limiting
      await new Promise((resolve) => setTimeout(resolve, 2000));
    }

    console.log('âœ… Scraping terminÃ©');
  } catch (error) {
    console.error('âŒ Erreur lors du scraping global:', error);
  }
}

/**
 * DÃ©marre le scheduler de scraping
 * Par dÃ©faut: toutes les 6 heures
 */
export function startScraperScheduler(db, schedule = '0 */6 * * *') {
  console.log(`â° Scheduler de scraping configurÃ©: ${schedule}`);

  cron.schedule(schedule, async () => {
    console.log('ğŸ• ExÃ©cution du job de scraping...');
    await scrapeAllEstablishments(db);
  });

  // ExÃ©cute immÃ©diatement au dÃ©marrage (optionnel)
  // scrapeAllEstablishments(db);
}

